# 보상 재설계

보상 값의 절대 크기가 과도하게 커지는 경우 MCTS의 백업 과정에서 값의 분산이 커져 탐색이 불안정해지고, 이후 강화학습 학습 과정에서도 그래디언트 스케일이 흔들리며 수렴이 불안정해지는 문제가 있었습니다. 이에 따라 보상이 일정 범위 내에서 동작하도록 스케일링을 적용하였고, 보상 값이 대략적으로 -5에서 5 범위로 들어오도록 scale factor를 추가하였습니다. 구현은 보상 함수를 100으로 나누는 방식으로 정규화하였습니다.

또한 장애물이 많아지거나 배치 가능한 공간이 감소할수록 MCTS가 수렴하지 못하는 문제가 관찰되었습니다. 원인을 분석한 결과, 현재 설비 배치에 성공한 경우 음수 보상을 받고 실패한 경우 0 보상을 받는 구조로 인해, 에이전트가 실패를 더 유리한 결과로 해석하는 역전 현상이 발생하고 있었습니다. 이에 따라 배치 실패 시에는 명시적으로 페널티를 부여하도록 보상 함수를 수정하였고, 더 이상 유효한 배치가 존재하지 않아 배치를 진행할 수 없는 상황에서는 아직 배치하지 못한 설비들의 면적 합에 비례하는 추가 페널티를 부여하도록 보상 함수를 재설계하였습니다.

$$
r_{\text{terminal}}
= -\lambda \cdot 
\frac{\sum_{g \in \text{remaining}} \left(w_g h_g\right)}
{\sum_{g \in \text{all}} \left(w_g h_g\right)}
$$


다만 배치 실패에 대한 페널티가 과도하게 커질 경우, 에이전트가 실패 가능성이 있는 구조 자체를 과도하게 회피하여 탐색이 보수적으로 변하고, 결과적으로 더 나은 배치를 발견할 수 있는 모험적 시도를 위축시킬 수 있다는 한계가 있습니다. 따라서 페널티 크기와 잔여 면적 페널티의 비율은 추가 실험을 통해 안정성과 탐색 성능의 균형이 맞는 구간을 탐색하며 조정해 나갈 계획입니다.


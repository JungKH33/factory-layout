# Alphachip 

앞을 한 수만 내다보는 greedy agent에 MCTS와 같은 탐색 알고리즘을 결합하면, 롤아웃을 통해 여러 수 이후의 결과까지 고려함으로써 보다 장기적인 의사결정이 가능함을 확인했습니다. 다만 이러한 탐색 기반 접근은 공장 크기가 커지고 배치해야 할 설비 수가 증가할수록, 의미 있는 의사결정을 위해 필요한 시뮬레이션 횟수가 급격히 늘어나 계산 비용이 커진다는 한계가 있습니다. 또한 탐색의 가지수를 줄이기 위해 초기 후보를 greedy가 좋아 보이는 위치 중심으로 제한하면, 탐색 자체가 해당 편향을 그대로 물려받아 여전히 local minima에 빠질 위험이 큽니다.

이러한 문제를 완화하기 위해 greedy 휴리스틱을 강화학습 기반 신경망 정책으로 대체할 수 있습니다. 단 한 수 앞의 즉시 보상만을 기준으로 선택하는 greedy와 달리, 정책 네트워크는 현재 배치 상태태와 제약, 그리고 앞으로 배치해야 할 설비들의 정보를 함께 입력으로 받아 장기적으로 유리한 배치를 직접 추정합니다. 즉, 탐색이 모든 경우를 시뮬레이션으로 확인하기 전에, 사람이 경험적으로 유망한 수를 먼저 떠올리는 것처럼 네트워크가 유망한 후보에 prior(사전확률/직관)를 부여해 탐색을 안내하는 역할을 수행한다고 볼 수 있습니다.

따라서 구글 딥마인드에서 2021년에 개발한 AlphaChip을 본 프로젝트에 맞게 구현해보았습니다. AlphaChip은 칩 설계에서 연결 관계를 나타내는 그래프 구조와 평면 배치 제약을 동시에 고려하고, 배치 과정을 순차적 의사결정 문제로 재구성해 강화학습으로 최적 해를 탐색하는 접근으로 알려져 있습니다. AlphaChip은 컴포넌트를 하나씩 배치해 나가는 순차적으로 배치하는데, 이떄 강화학습 에이전트는 현재까지의 배치 상태를 관측으로 받아 이미 배치된 요소와 남은 요소, 요소 간 연결 관계, 혼잡도와 거리 신호 등을 종합적으로 반영하고, 다음에 배치할 요소의 위치를 선택하는 정책을 학습합니다.

공장 설비 배치 문제는 반도체 플로어플래닝과 구조적으로 유사한 점이 많다고 판단하였습니다. 칩 설계에서 매크로 간 연결이 배치 품질에 큰 영향을 주는 것처럼, 공장 배치에서도 설비 간 물류 흐름과 다양한 제약 조건이 최종 배치 품질에 직접적으로 작용합니다. 따라서 그래프 기반의 관계와 2차원 평면 배치를 함께 다루는 AlphaChip의 문제 설정과 최적화 방식은 본 프로젝트에도 충분히 이식 가능하다고 보았습니다.

다만 구글 딥마인드에서 공개한 공식 오픈소스 (https://github.com/google-research/circuit_training)는 TensorFlow와 TPU 기반으로 구성되어 있어 범용적인 환경에서 활용하거나 다른 모델 및 파이프라인과 결합하는 데 제약이 있었습니다. 이에 따라 공개 구현을 참고하되, 실행 환경과 확장성을 고려하여 PyTorch와 GPU 기반 코드로 전면 리팩토링을 진행하였습니다.

또한 리팩토링한 구현을 기존에 사용하던 공장 배치 환경과 연동하여, 학습과 평가가 동일한 인터페이스에서 동작하도록 통합하였습니다. 이 과정에서 AlphaChip에서 활용된 범용적인 아이디어 중 하나인 배치 순서를 큰 설비부터 우선 배치하는 전략이 공장 배치뿐 아니라 다른 최적화 모델에도 적용 가능하다고 판단하였고, 이를 환경 코드의 기본 정책으로 반영하였습니다. 추가로 면적이 동일한 설비가 존재하는 경우에는 그래프 연결 구조를 기준으로 우선순위를 결정하도록 하여, 물류 흐름과 연관성이 큰 설비가 먼저 배치되도록 정렬 규칙을 구성하였습니다.